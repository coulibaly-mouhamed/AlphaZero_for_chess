{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfLq4D_NkSgW",
        "outputId": "417bdc7a-6568-4431-b55a-faaf335963aa"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!pip install open_spiel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UptA7e5RkM2X"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Implement the AlphaZero algorithm with PyTorch for the Chess game. \n",
        "'''\n",
        "from random import sample\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "'''\n",
        "Create a convolutional block made of :\n",
        "    -A convolution of 256 filters of kernel size 3x3 with stride 1 \n",
        "    - batch normalization\n",
        "    -A rectifier non linearity \n",
        "'''\n",
        "import torch.nn.functional as F\n",
        "import random \n",
        "import pyspiel\n",
        "import numpy as np\n",
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n",
        "class Node:\n",
        "    \n",
        "    def __init__(self,state,obs,player_root_node,parent,prior_probability,player_turn,alphazero_network):\n",
        "        '''\n",
        "        Node class that describes a node in the MCTS tree. It contains:\n",
        "            -state: the state of the node its a pyspiel state\n",
        "            -obs : representation of the state of the board in tensor of shape (21,8,8)\n",
        "            -player_root_node: the player that is playing the root node\n",
        "            -parent: the parent node of the current node\n",
        "            -prior_probability: the prior probability of the node\n",
        "            -player_turn: the player that is playing the current node\n",
        "            -alphazero_network: the neural network used to compute the prior probability and the value of the node\n",
        "            -children: a dictionary that contains the children of the node\n",
        "            -cput: the exploration parameter\n",
        "            -is_leaf: boolean that indicates if the node is a leaf or not\n",
        "            -visit_count: the number of times the node has been visited\n",
        "            -tot_act_value: the total action value of the node\n",
        "            -mean_value: the mean value of the node\n",
        "        '''\n",
        "        \n",
        "        self.state = state\n",
        "        self.obs = obs\n",
        "        \n",
        "        self.children = {}\n",
        "        self.player_root_node = player_root_node\n",
        "        self.parent = parent #[] if the node is the root node\n",
        "        \n",
        "        self.player_turn = player_turn\n",
        "        self.neural_network = alphazero_network\n",
        "        \n",
        "        self.prior_probability = prior_probability\n",
        "        self.visit_count = 0.\n",
        "        self.tot_act_value = 0.\n",
        "        self.mean_value = 0.\n",
        "        \n",
        "        \n",
        "        self.is_leaf = True\n",
        "        \n",
        "    def expand(self):\n",
        "        '''\n",
        "            Expansion of the node by creating its children\n",
        "        '''\n",
        "        \n",
        "        if (self.state.is_terminal()):\n",
        "            raise Exception('Cannot expand a terminal node!!!')\n",
        "        \n",
        "        #Compute the prior probability of the children nodes\n",
        "        prior_probability_child = self.neural_network.forward(self.obs)[0]\n",
        "        prior_probability_child = prior_probability_child.cpu().detach().numpy() #prior_probability_child.flatten().detach().numpy() #convert to numpy array\n",
        "        prior_probability_child = prior_probability_child.flatten()\n",
        "        #Get the legal actions of the node\n",
        "        legal_actions = self.state.legal_actions()\n",
        "        \n",
        "        #Loop over the legal actions and create the children nodes that come from the legal actions\n",
        "        \n",
        "        for id_action in range(len(legal_actions)):\n",
        "            \n",
        "            child_state = self.state.clone() #clone the state of the node\n",
        "            \n",
        "            child_state.apply_action(legal_actions[id_action]) #apply the action to the state\n",
        "            \n",
        "            if (not child_state.is_terminal()):\n",
        "                obs_child = child_state.observation_tensor() #get the observation tensor of the child state\n",
        "                \n",
        "            else:\n",
        "                obs_child = None  \n",
        "                player_turn = None \n",
        "                   \n",
        "            \n",
        "            player_root_node = self.player_root_node #get the player that played the root node\n",
        "                \n",
        "            prior_probability_child_node = prior_probability_child[id_action] #get the prior probability of the child node\n",
        "                \n",
        "            player_turn = child_state.current_player() #get the player that is playing the child node\n",
        "                \n",
        "            self.children[id_action] = Node(child_state,obs_child,player_root_node,self,prior_probability_child_node,player_turn,self.neural_network) #create the child node\n",
        "            \n",
        "        self.is_leaf = False\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    \n",
        "    def ucb_score(self,id_child,cput):\n",
        "        \n",
        "        '''\n",
        "            Compute the ucb score of a child node given its id_child\n",
        "        '''\n",
        "        \n",
        "        #Get the child node\n",
        "        child_node = self.children[id_child]\n",
        "        \n",
        "        #Compute the ucb score\n",
        "        ucb_score = child_node.mean_value + cput*child_node.prior_probability*np.sqrt(self.visit_count)/(1+child_node.visit_count)\n",
        "        \n",
        "        return ucb_score\n",
        "     \n",
        "        \n",
        "    def select_child(self,cput):\n",
        "        '''\n",
        "            Select the child node with the highest ucb score\n",
        "        '''\n",
        "        #First check if there are children nodes\n",
        "        \n",
        "        if self.children == {}:\n",
        "            raise Exception(\"The node has no children\")\n",
        "        \n",
        "        else:\n",
        "            #Get the ucb score of each child node\n",
        "            nb_children = len(self.children)\n",
        "            ucb_scores = [self.ucb_score(id_child,cput) for id_child in range(nb_children)]\n",
        "            \n",
        "            #Select the child node with the highest ucb score\n",
        "            id_best_child = np.argmax(ucb_scores)\n",
        "            \n",
        "            return  self.children[id_best_child]\n",
        "     \n",
        "     \n",
        "        \n",
        "    def update_node(self,value):\n",
        "        ''' \n",
        "            Update information of the node \n",
        "        '''\n",
        "        #Update the node information\n",
        "        \n",
        "        self.visit_count += 1\n",
        "        self.tot_act_value += value\n",
        "        self.mean_value = self.tot_act_value/self.visit_count\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def summary(self):\n",
        "        '''\n",
        "            Print the summary of the node\n",
        "        '''\n",
        "        print(\"Node summary...........\")\n",
        "        print(\"Player turn: \",self.player_turn)\n",
        "        print(\"Visit count: \",self.visit_count)\n",
        "        print(\"Mean action value: \",self.mean_value)\n",
        "        print(\"Prior probability: \",self.prior_probability)\n",
        "        print(\"Is leaf: \",self.is_leaf)\n",
        "        print(\"Is terminal: \",self.state.is_terminal())\n",
        "        print('------------------------------')\n",
        "        return\n",
        "    \n",
        "    \n",
        "def update_path(path,value):\n",
        "        '''\n",
        "            Update the information of each node from the current node to the root node\n",
        "                Input:\n",
        "                path = list of nodes from the current node to the root node\n",
        "        '''\n",
        "        #Convert value to a numpy array\n",
        "        \n",
        "        \n",
        "        for node in path:\n",
        "            node.update_node(value)\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def compute_pi_posterior(root_node,temperature):\n",
        "    '''\n",
        "    Compute the posterior probability of the root node\n",
        "        root_node: the root node of the tree\n",
        "        temperature: the temperature parameter\n",
        "    \n",
        "    '''\n",
        "    inv_temp = 1/temperature\n",
        "    pi = torch.zeros(4672)\n",
        "    pi.to('cuda')\n",
        "    actions_list = root_node.state.legal_actions()\n",
        "    for id_action in range(len(actions_list)):\n",
        "        pi[actions_list[id_action]] = root_node.children[id_action].visit_count#**inv_temp\n",
        "        pi[actions_list[id_action]]= pi[actions_list[id_action]]/(root_node.visit_count)#**inv_temp)\n",
        "    #Reset and delete the children nodes\n",
        "    \n",
        "    del root_node.children \n",
        "    return pi\n",
        "  \n",
        "    \n",
        "def MCTS(root,num_simulations):\n",
        "    '''\n",
        "    Perform a Monte Carlo Tree Search algorithm for the game of chess\n",
        "    root: the root node of the tree\n",
        "    num_simulations: the number of simulations to perform\n",
        "    actions_tracker: the list of actions that have been taken to reach the root node\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    #First Expansion\n",
        "    root.expand()\n",
        "    cput = 1e4 # On choisit grand pour favoriser l'exploration et les outputs du réseaux de neurones\n",
        "    \n",
        "    for i in range(num_simulations):\n",
        "        #print('Simulation number: ',i)\n",
        "        if (i<=num_simulations*(2/3)):\n",
        "            cput = cput*0.99\n",
        "        else:\n",
        "            cput = 1\n",
        "        search_path = []\n",
        "        search_path.append(root)\n",
        "        \n",
        "        #Select the child of the root node with the best ucb score\n",
        "        current = root.select_child(cput)\n",
        "        search_path.append(current)\n",
        "        \n",
        "        #Selection\n",
        "        is_leaf = current.is_leaf\n",
        "        while(not is_leaf):\n",
        "            #browse the tree by selecting the best child\n",
        "            \n",
        "            current = current.select_child(cput)\n",
        "            is_leaf = current.is_leaf \n",
        "            search_path.append(current)  \n",
        "            \n",
        "        #Expansion\n",
        "     \n",
        "        if (current.visit_count==0):\n",
        "            if (not current.state.is_terminal()):\n",
        "                prior_prob,value = current.neural_network.forward(current.obs) #rollout\n",
        "                #Convert value to numpy float\n",
        "                value = value.cpu().detach().numpy()[0][0]\n",
        "                if (current.player_turn== root.player_turn):    #Backpropagation\n",
        "                        update_path(search_path,value)\n",
        "                else:\n",
        "                    update_path(search_path,-value)\n",
        "            \n",
        "            else:\n",
        "                value = current.state.player_reward(root.player_turn)\n",
        "                update_path(search_path,value)\n",
        "            \n",
        "            \n",
        "                \n",
        "        else:\n",
        "            \n",
        "            \n",
        "            #Simulation\n",
        "            if (current.state.is_terminal()):\n",
        "               \n",
        "                value = current.state.player_reward(root.player_turn)\n",
        "                #update_path(search_path,value) Don't need to update the path because the node is terminal and has already be updated\n",
        "                \n",
        "            \n",
        "            else:\n",
        "                current.expand()\n",
        "                current = current.select_child(cput)\n",
        "                search_path.append(current)\n",
        "                \n",
        "                if (not current.state.is_terminal()):\n",
        "                    prior_prob,value = current.neural_network.forward(current.obs) #rollout\n",
        "                    value = value.cpu().detach().numpy()[0][0]\n",
        "                    if (current.player_turn== root.player_turn):    #Backpropagation\n",
        "                        update_path(search_path,value)\n",
        "                    else:\n",
        "                        update_path(search_path,-value)\n",
        "            \n",
        "                else:\n",
        "                    \n",
        "                    value = current.state.player_reward(root.player_turn) #Vérifier si c'est le bon player turn mieux si on a id_winner\n",
        "                    update_path(search_path,value)\n",
        "                    \n",
        "                \n",
        "        #print('*********************************')\n",
        "    temperature = 1\n",
        "    #print('Loop done computing pi  ')\n",
        "    pi =compute_pi_posterior(root,temperature)   \n",
        "    \n",
        "    return pi#,root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def choose_action(pi,state):\n",
        "    '''\n",
        "        Choose an action according to the probability distribution pi\n",
        "        Input:\n",
        "            -pi: probability distribution\n",
        "        Output:\n",
        "            -id_action: the id of the chosen action\n",
        "    '''\n",
        "    id_action = 5000\n",
        "    try_ = 0\n",
        "    #convert pi to numpy array\n",
        "    pi_np = pi.detach().numpy()\n",
        "    actions  = state.legal_actions()\n",
        "    while (id_action not in actions):\n",
        "    \n",
        "        id_action = np.random.choice(len(pi),p=pi_np)\n",
        "        try_ += 1\n",
        "    #print('Find action after ',try_,' tries')\n",
        "    return id_action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.action_size = 8*8*73\n",
        "        self.conv1 = nn.Conv2d(20, 256, 3, stride=1, padding=1) #change 22 par 21\n",
        "        self.bn1 = nn.BatchNorm2d(256)\n",
        "\n",
        "    def forward(self, s):\n",
        "        # Convert to tensor\n",
        "        s = torch.tensor(s, dtype=torch.float32) \n",
        "        s = s.view(-1, 20,8,8)  # batch_size x channels x board_x x board_y\n",
        "        s = s.to('cuda')\n",
        "        s = F.relu(self.bn1(self.conv1(s)))\n",
        "        return s\n",
        "  \n",
        "'''\n",
        "Create a residual block made of :\n",
        "    -A convolution of 2 filters of kernel size 1*1 with stride 1\n",
        "    - batch normalization\n",
        "    - A skip connection that adds the input to the block\n",
        "    -A rectifier non linearity\n",
        "'''\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, inplanes=256, planes=256, stride=1, downsample=None):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "    \n",
        "\n",
        "class OutBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OutBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(256, 1, kernel_size=1) # value head\n",
        "        self.bn = nn.BatchNorm2d(1)\n",
        "        self.fc1 = nn.Linear(8*8, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(256, 128, kernel_size=1) # policy head\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        self.fc = nn.Linear(8*8*128, 8*8*73)\n",
        "    \n",
        "    def forward(self,s):\n",
        "        v = F.relu(self.bn(self.conv(s))) # value head\n",
        "        v = v.view(-1, 8*8)  # batch_size X channel X height X width\n",
        "        v = F.relu(self.fc1(v))\n",
        "        v = torch.tanh(self.fc2(v))\n",
        "        \n",
        "        p = F.relu(self.bn1(self.conv1(s))) # policy head\n",
        "        p = p.view(-1, 8*8*128)\n",
        "        p = self.fc(p)\n",
        "        p = self.logsoftmax(p).exp()\n",
        "        return p, v\n",
        "        \n",
        "class Alphazero_net(nn.Module):\n",
        "    '''\n",
        "    Implement the AlphaZero algorithm with PyTorch for the Chess game. It consists in a \n",
        "    Residual Network with 19 layers and 256 filters.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Alphazero_net, self).__init__()\n",
        "        self.conv = ConvBlock()\n",
        "        for block in range(19):\n",
        "            self.add_module('resblock'+str(block),ResBlock())\n",
        "        self.outblock = OutBlock()\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=400, gamma=0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        s = self.conv(x)\n",
        "        for block in range(19):\n",
        "            s = self.__getattr__('resblock'+str(block))(s)\n",
        "        p,v = self.outblock(s)\n",
        "        return p,v\n",
        "    \n",
        "    def checkpoint(self,epoch):\n",
        "        torch.save(self.state_dict(), 'checkpoint.pth.tar')\n",
        "        print('Checkpoint saved !')\n",
        "        \n",
        "    def loss_function(self,p,v,pi,z):\n",
        "        '''\n",
        "        Compute the loss function of the AlphaZero algorithm which is the sum of the \n",
        "        cross entropy loss and the MSE loss.\n",
        "        \n",
        "        '''\n",
        "        return -torch.sum(pi*torch.log(p)) + torch.sum((z-v)**2)\n",
        "    \n",
        "     \n",
        "    def fit_to_self_play(self,data_batch,cpu=1):\n",
        "        '''\n",
        "        Training Pipeline of the AlphaZero algorithm which consists in :\n",
        "            - From data generated by self play do\n",
        "            - Predict the output of the neural network\n",
        "            - Compute the loss function\n",
        "            - Backpropagate the loss function\n",
        "            - Update the weights of the neural network\n",
        "        '''\n",
        "        \n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss_batch = 0\n",
        "        \n",
        "        for state_board in data_batch:\n",
        "            state, pi_board, v_board = state_board\n",
        "            p_predicted, v_predicted = self.forward(state)\n",
        "            loss_batch += self.loss_function(p_predicted, v_predicted, pi_board, v_board)\n",
        "\n",
        "        loss_batch.backward()\n",
        "        self.optimizer.step()\n",
        "            \n",
        "        return loss_batch\n",
        "    \n",
        "    \n",
        "    def evaluator (self):\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def self_play(self,n_simu):\n",
        "        '''\n",
        "        Simulation of a single game with MCTS guided by a neural network \n",
        "        Inputs:\n",
        "            -neural_network \n",
        "        Ouputs: \n",
        "            - Data_frame with for each state of the game the vector of probability pi \n",
        "        '''\n",
        "        state_history = []\n",
        "        pi_history = []\n",
        "        \n",
        "        #Initialization of state_history and pi_history\n",
        "        #state_history.append(obs_init)\n",
        "    \n",
        "        #Initialisation of the root node\n",
        "        neural_network = self\n",
        "        neural_network = neural_network.to('cuda')\n",
        "        game = pyspiel.load_game(\"chess\")\n",
        "        state = game.new_initial_state()\n",
        "        obs_root = state.observation_tensor()\n",
        "        player_turn = state.current_player()\n",
        "        player_turn_root = player_turn\n",
        "        parent =[]\n",
        "        prob =1\n",
        "        root = Node(state,obs_root,player_turn_root,parent,prob,player_turn,neural_network)\n",
        "    \n",
        "        play =0\n",
        "        is_terminal = root.state.is_terminal()\n",
        "        \n",
        "        state_history.append(obs_root)\n",
        "        while not is_terminal:\n",
        "            if (play%50==0):\n",
        "                print(\"play = \",play)\n",
        "            #Run MCTS\n",
        "            #print('Run MCTS')\n",
        "            pi = MCTS(root,n_simu)\n",
        "            #print('End MCTS')\n",
        "            \n",
        "            pi_history.append(pi.cpu().detach().numpy())\n",
        "            #Select the action\n",
        "            id_action = choose_action(pi,root.state)\n",
        "            #Update the environnement and play the action\n",
        "            root.state.apply_action(id_action)\n",
        "            \n",
        "            next_root_state = root.state.clone()\n",
        "            if (next_root_state.is_terminal()):\n",
        "                print('Game over')\n",
        "                break\n",
        "            player_turn = next_root_state.current_player()\n",
        "            parent = []\n",
        "            obs_root = next_root_state.observation_tensor()\n",
        "            root = Node(next_root_state,obs_root,player_turn_root,parent,prob,player_turn,neural_network)\n",
        "            \n",
        "            is_terminal = root.state.is_terminal()\n",
        "        \n",
        "            \n",
        "            play += 1\n",
        "            #print('___________________________________________________________________')\n",
        "            \n",
        "            \n",
        "        reward = root.state.player_reward(player_turn_root)\n",
        "        \n",
        "        #Convert state_history to tensor\n",
        "        state_history = torch.tensor(state_history)\n",
        "        state_history = state_history.to('cuda')\n",
        "        #Convert pi_history to tensor\n",
        "        pi_history = torch.tensor(pi_history)\n",
        "        pi_history = pi_history.to('cuda')\n",
        "        #Convert reward to tensor\n",
        "        reward = torch.tensor(reward)\n",
        "        reward = reward.to('cuda')\n",
        "        return state_history,pi_history,reward\n",
        "\n",
        "    \n",
        "    def sample(self,data,batch_size):\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def update_parameters(self,data,batch_size=500,epochs=50):  \n",
        "        for i in range(epochs):\n",
        "            data_batch = sample(data,batch_size)\n",
        "            loss_batch = self.fit_to_self_play(data_batch)\n",
        "    \n",
        "    def self_play(self):\n",
        "        pass\n",
        "    \n",
        "    def train(self,num_iterations=1000):\n",
        "        '''\n",
        "        Training Pipeline of the AlphaZero algorithm which consists in :\n",
        "            - Generate data from self play games monitored by an MCTS\n",
        "            - Update the parameters of the neural network\n",
        "        '''\n",
        "        '''\n",
        "        for i in range(num_iterations):\n",
        "            \n",
        "            data = self.self_play()\n",
        "            \n",
        "            self.update_parameters(data)\n",
        "            \n",
        "            if i%50 == 0:\n",
        "                self.checkpoint()\n",
        "        '''\n",
        "        \n",
        "\n",
        "\n",
        "###############################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js1HuO3bjw96",
        "outputId": "82186d10-30a7-40c9-8e5d-4330f9d1f506"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "'''\n",
        "    In this script we will implement the self play method based on MCTS algorithm guided by a neural network\n",
        "    \n",
        "'''\n",
        "directory = os.getcwd()\n",
        "\n",
        "\n",
        "def play_mcts_guided_game(neural_network,n_simu,num_game):\n",
        "    '''\n",
        "        Simulation of a single game with MCTS guided by a neural network \n",
        "        Inputs:\n",
        "            -neural_network \n",
        "        Ouputs: \n",
        "            - Data_frame with for each state of the game the vector of probability pi \n",
        "    '''\n",
        "    state_history = []\n",
        "    pi_history = []\n",
        "    \n",
        "    #Initialization of state_history and pi_history\n",
        "    #state_history.append(obs_init)\n",
        "   \n",
        "    #Initialisation of the root node\n",
        "    neural_network = Alphazero_net()\n",
        "    neural_network = neural_network.to('cuda')\n",
        "    game = pyspiel.load_game(\"chess\")\n",
        "    state = game.new_initial_state()\n",
        "    obs_root = state.observation_tensor()\n",
        "    player_turn = state.current_player()\n",
        "    player_turn_root = player_turn\n",
        "    parent =[]\n",
        "    prob =1\n",
        "    root = Node(state,obs_root,player_turn_root,parent,prob,player_turn,neural_network)\n",
        "   \n",
        "    play =0\n",
        "    is_terminal = root.state.is_terminal()\n",
        "    \n",
        "    state_history.append(obs_root)\n",
        "    while not is_terminal:\n",
        "        if (play%50==0):\n",
        "          print(\"play = \",play)\n",
        "        #Run MCTS\n",
        "        #print('Run MCTS')\n",
        "        pi = MCTS(root,n_simu)\n",
        "        #print('End MCTS')\n",
        "        \n",
        "        pi_history.append(pi.cpu().detach().numpy())\n",
        "        #Select the action\n",
        "        '''\n",
        "        if  (pi.sum() != 1):\n",
        "            print(\"pi.sum() = \",pi.sum())\n",
        "            print('Problem with pi')\n",
        "            #Print pi \n",
        "            break\n",
        "        '''\n",
        "        id_action = choose_action(pi,root.state)\n",
        "        #Update the environnement and play the action\n",
        "        root.state.apply_action(id_action)\n",
        "        \n",
        "        next_root_state = root.state.clone()\n",
        "        if (next_root_state.is_terminal()):\n",
        "            print('Game over')\n",
        "            break\n",
        "        player_turn = next_root_state.current_player()\n",
        "        parent = []\n",
        "        obs_root = next_root_state.observation_tensor()\n",
        "        root = Node(next_root_state,obs_root,player_turn_root,parent,prob,player_turn,neural_network)\n",
        "        \n",
        "        is_terminal = root.state.is_terminal()\n",
        "       \n",
        "        \n",
        "        play += 1\n",
        "        #print('___________________________________________________________________')\n",
        "        \n",
        "         \n",
        "    reward = root.state.player_reward(player_turn_root)\n",
        "    try:\n",
        "      os.chdir(directory+'/data_self_play')\n",
        "    except:\n",
        "      os.mkdir('data_self_play')\n",
        "      os.chdir(directory+'/data_self_play')\n",
        "        #Create a folder for the game\n",
        "    os.mkdir('game_'+str(num_game))\n",
        "        #Go to the folder game_num_game\n",
        "    os.chdir(directory+'/data_self_play/game_'+str(num_game))\n",
        "    #Save the neural network weights\n",
        "    torch.save(neural_network.state_dict(), 'neural_network_weights'+str(num_game)+'.pt')\n",
        "    #Save the state history and the pi history and the reward\n",
        "     #Convert the state history to numpy array\n",
        "    state_history_np = np.array(state_history)\n",
        "     #Convert the pi history to numpy array\n",
        "    \n",
        "    pi_history_np = np.array(pi_history)\n",
        "    #Save the state history and the pi history\n",
        "    np.save('state_history'+str(num_game)+'.npy',state_history_np)\n",
        "    #print(pi_history_np.shape)\n",
        "    np.save('pi_history'+str(num_game)+'.npy',pi_history_np)\n",
        "    np.save('reward'+str(num_game)+'.npy',reward)\n",
        "    \n",
        "    os.chdir(directory)\n",
        "\n",
        "    print('Game number ',num_game,' is over')\n",
        "    print('Reward = ',reward)\n",
        "    return #state_history,pi_history,reward\n",
        "\n",
        "#neural_network = Alphazero_net()\n",
        "#for game in range (1):\n",
        "#    print('Game number: ',game)\n",
        "#    play_mcts_guided_game(neural_network,150,game)\n",
        "#    print('-------------------------------------------------------------------')\n",
        "#print('Reward: ',reward)\n",
        "#print('End of the games')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLes cellules en cours d’exécution avec Python 3.7.9 nécessitent ipykernel package.\n",
            "\u001b[1;31mExécutez la commande suivante pour installer 'ipykernel' dans l’environnement Python. \n",
            "\u001b[1;31mCommande : 'conda install -p /opt/intel/oneapi/intelpython/latest ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "\n",
        "neural_network = Alphazero_net()\n",
        "neural_network.self_play(10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "de7c8584ae51fbb089eecc0d6f203cc3cae3114d5c45556091d2c5b63465aa84"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
